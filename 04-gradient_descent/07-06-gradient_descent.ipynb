{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2626, 0.4638, 0.3145],\n",
       "        [0.1725, 0.5353, 0.3530],\n",
       "        [0.1292, 0.9595, 0.1822]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1196, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 7.2380e-02\n",
      "tensor([[0.2264, 0.4052, 0.3113],\n",
      "        [0.2230, 0.5275, 0.4079],\n",
      "        [0.2560, 0.9241, 0.3417]], requires_grad=True)\n",
      "2-th Loss: 4.3785e-02\n",
      "tensor([[0.1983, 0.3596, 0.3088],\n",
      "        [0.2624, 0.5214, 0.4506],\n",
      "        [0.3547, 0.8965, 0.4658]], requires_grad=True)\n",
      "3-th Loss: 2.6487e-02\n",
      "tensor([[0.1765, 0.3241, 0.3068],\n",
      "        [0.2929, 0.5166, 0.4838],\n",
      "        [0.4314, 0.8751, 0.5623]], requires_grad=True)\n",
      "4-th Loss: 1.6023e-02\n",
      "tensor([[0.1595, 0.2965, 0.3053],\n",
      "        [0.3167, 0.5129, 0.5096],\n",
      "        [0.4911, 0.8584, 0.6373]], requires_grad=True)\n",
      "5-th Loss: 9.6931e-03\n",
      "tensor([[0.1463, 0.2751, 0.3041],\n",
      "        [0.3352, 0.5100, 0.5297],\n",
      "        [0.5375, 0.8454, 0.6957]], requires_grad=True)\n",
      "6-th Loss: 5.8637e-03\n",
      "tensor([[0.1360, 0.2584, 0.3032],\n",
      "        [0.3496, 0.5078, 0.5453],\n",
      "        [0.5736, 0.8353, 0.7411]], requires_grad=True)\n",
      "7-th Loss: 3.5472e-03\n",
      "tensor([[0.1280, 0.2454, 0.3025],\n",
      "        [0.3608, 0.5061, 0.5575],\n",
      "        [0.6017, 0.8275, 0.7764]], requires_grad=True)\n",
      "8-th Loss: 2.1458e-03\n",
      "tensor([[0.1218, 0.2353, 0.3019],\n",
      "        [0.3695, 0.5047, 0.5669],\n",
      "        [0.6236, 0.8214, 0.8039]], requires_grad=True)\n",
      "9-th Loss: 1.2981e-03\n",
      "tensor([[0.1169, 0.2275, 0.3015],\n",
      "        [0.3763, 0.5037, 0.5743],\n",
      "        [0.6405, 0.8166, 0.8252]], requires_grad=True)\n",
      "10-th Loss: 7.8527e-04\n",
      "tensor([[0.1132, 0.2214, 0.3012],\n",
      "        [0.3816, 0.5029, 0.5800],\n",
      "        [0.6538, 0.8129, 0.8418]], requires_grad=True)\n",
      "11-th Loss: 4.7504e-04\n",
      "tensor([[0.1102, 0.2166, 0.3009],\n",
      "        [0.3857, 0.5022, 0.5844],\n",
      "        [0.6640, 0.8101, 0.8548]], requires_grad=True)\n",
      "12-th Loss: 2.8737e-04\n",
      "tensor([[0.1080, 0.2129, 0.3007],\n",
      "        [0.3888, 0.5017, 0.5879],\n",
      "        [0.6720, 0.8078, 0.8648]], requires_grad=True)\n",
      "13-th Loss: 1.7384e-04\n",
      "tensor([[0.1062, 0.2101, 0.3006],\n",
      "        [0.3913, 0.5013, 0.5906],\n",
      "        [0.6782, 0.8061, 0.8726]], requires_grad=True)\n",
      "14-th Loss: 1.0516e-04\n",
      "tensor([[0.1048, 0.2078, 0.3004],\n",
      "        [0.3933, 0.5010, 0.5927],\n",
      "        [0.6831, 0.8047, 0.8787]], requires_grad=True)\n",
      "15-th Loss: 6.3617e-05\n",
      "tensor([[0.1037, 0.2061, 0.3003],\n",
      "        [0.3948, 0.5008, 0.5943],\n",
      "        [0.6868, 0.8037, 0.8834]], requires_grad=True)\n",
      "16-th Loss: 3.8484e-05\n",
      "tensor([[0.1029, 0.2047, 0.3003],\n",
      "        [0.3959, 0.5006, 0.5956],\n",
      "        [0.6898, 0.8029, 0.8871]], requires_grad=True)\n",
      "17-th Loss: 2.3281e-05\n",
      "tensor([[0.1023, 0.2037, 0.3002],\n",
      "        [0.3968, 0.5005, 0.5966],\n",
      "        [0.6920, 0.8022, 0.8900]], requires_grad=True)\n",
      "18-th Loss: 1.4083e-05\n",
      "tensor([[0.1018, 0.2029, 0.3002],\n",
      "        [0.3975, 0.5004, 0.5973],\n",
      "        [0.6938, 0.8017, 0.8922]], requires_grad=True)\n",
      "19-th Loss: 8.5194e-06\n",
      "tensor([[0.1014, 0.2022, 0.3001],\n",
      "        [0.3981, 0.5003, 0.5979],\n",
      "        [0.6952, 0.8013, 0.8939]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() # Calculate gradients.\n",
    "\n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    # You don't need to aware this now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
